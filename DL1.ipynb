{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anamunoz01/DEEP_LEARNING_4_UFV/blob/main/DL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "#keras.version_\n",
        "import pandas as pd\n",
        "df = pd.read_csv('hola.csv')"
      ],
      "metadata": {
        "id": "q4aD1s_aiOQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "VJLcA2DQmuDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[['sourceText','translatedText']]"
      ],
      "metadata": {
        "id": "ENZrElSjm44w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jgT0fOoFoaur",
        "outputId": "9a6e72d3-6340-4e55-cf4f-7695cd07bfc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sourceText  \\\n",
              "0     Kaisa-Leena Mäkäräinen (born 11 January 1983) ...   \n",
              "1     Outside sports, Mäkäräinen is currently studyi...   \n",
              "2     Her team coach is Jonne Kähkönen, while Jarmo ...   \n",
              "3     Mäkäräinen was originally a cross-country skie...   \n",
              "4        She started training for the biathlon in 2003.   \n",
              "...                                                 ...   \n",
              "1466  Speaking to Madrid-based Diario AS in 2013 abo...   \n",
              "1467  Rossell proceeded to try again first under San...   \n",
              "1468  In the documentary \"Un Sueño Real\", she reveal...   \n",
              "1469                  Her struggle proved unsuccessful.   \n",
              "1470  It wasn't until 2013, in Perez's second stint ...   \n",
              "\n",
              "                                         translatedText  \n",
              "0     Kaisa-Leena Mäkäräinen (nacida el 11 de enero ...  \n",
              "1     Además de los deportes, estudia actualmente en...  \n",
              "2     El entrenador de su equipo es Jonne Kähkönen, ...  \n",
              "3     Mäkäräinen era originalmente esquiadora de cam...  \n",
              "4           Comenzó a entrenar para el biatlón en 2003.  \n",
              "...                                                 ...  \n",
              "1466  En 2013, habló de sus primeras frustraciones c...  \n",
              "1467  Rossell volvió a intentarlo con el sucesor de ...  \n",
              "1468  En el documental «Un sueño real», contó que se...  \n",
              "1469                            Su intento fue en vano.  \n",
              "1470  Recién en 2013, en el segundo mandato de Pérez...  \n",
              "\n",
              "[1471 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-629c7c88-6102-49fc-a2c0-b2d98a1b5d2a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sourceText</th>\n",
              "      <th>translatedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kaisa-Leena Mäkäräinen (born 11 January 1983) ...</td>\n",
              "      <td>Kaisa-Leena Mäkäräinen (nacida el 11 de enero ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outside sports, Mäkäräinen is currently studyi...</td>\n",
              "      <td>Además de los deportes, estudia actualmente en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Her team coach is Jonne Kähkönen, while Jarmo ...</td>\n",
              "      <td>El entrenador de su equipo es Jonne Kähkönen, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mäkäräinen was originally a cross-country skie...</td>\n",
              "      <td>Mäkäräinen era originalmente esquiadora de cam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>She started training for the biathlon in 2003.</td>\n",
              "      <td>Comenzó a entrenar para el biatlón en 2003.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>Speaking to Madrid-based Diario AS in 2013 abo...</td>\n",
              "      <td>En 2013, habló de sus primeras frustraciones c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1467</th>\n",
              "      <td>Rossell proceeded to try again first under San...</td>\n",
              "      <td>Rossell volvió a intentarlo con el sucesor de ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>In the documentary \"Un Sueño Real\", she reveal...</td>\n",
              "      <td>En el documental «Un sueño real», contó que se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>Her struggle proved unsuccessful.</td>\n",
              "      <td>Su intento fue en vano.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>It wasn't until 2013, in Perez's second stint ...</td>\n",
              "      <td>Recién en 2013, en el segundo mandato de Pérez...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1471 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-629c7c88-6102-49fc-a2c0-b2d98a1b5d2a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-629c7c88-6102-49fc-a2c0-b2d98a1b5d2a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-629c7c88-6102-49fc-a2c0-b2d98a1b5d2a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['translatedText'] = df1['translatedText'].apply(lambda x: \"[start] \" + x + \" [end]\")\n"
      ],
      "metadata": {
        "id": "ogQiADzTpd09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista = df1.to_numpy()\n",
        "lista = lista.tolist"
      ],
      "metadata": {
        "id": "bfeCSTRB2JOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista = lista.tolist()\n",
        "lista"
      ],
      "metadata": {
        "id": "YIL7Bycy2kOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print(random.choice(lista))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLcxudKq2-el",
        "outputId": "6ae48093-911a-45fb-9ad5-dffb629b5608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Despite finding most of these zoology lectures \"quite boring\", his interest was piqued by the neurophysiology course taught by Mitsuo Tamashige.', '[start] A pesar de que estas clases de zoología le resultaban \"bastante aburridas\", un curso de Neurofisiología dictado por Mitsuo Tamashige hizo que aumentara su interés. [end]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(lista)\n",
        "# 15% validación\n",
        "num_val_samples = int(0.15 * len(lista))\n",
        "# 70% entrenamiento\n",
        "num_train_samples = len(lista) - 2 * num_val_samples\n",
        "train_pairs = lista[:num_train_samples]\n",
        "val_pairs = lista[num_train_samples:num_train_samples + num_val_samples]\n",
        "# 15% test o pruebas\n",
        "test_pairs = lista[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "tHXRMoXm3ohN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Prepara una función de estandarización de cadenas \n",
        "# personalizada para la capa TextVectorization en \n",
        "# español: conserva [ y ] pero elimina ¿ (así como \n",
        "# todos los demás caracteres de cadenas.puntuación)\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "print(strip_chars)\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "print(strip_chars)\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "print(strip_chars)\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# Para mantener las cosas simples, solo veremos las \n",
        "# 15.000 palabras principales en cada idioma y \n",
        "# restringiremos las oraciones a 20 palabras.\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "# La capa en Inglés\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "# La capa en Español\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    # Generamos oraciones en español que tengan un token \n",
        "    # adicional, ya que necesitaremos compensar la oración \n",
        "    # en un paso durante el entrenamiento\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# Aprende el vocabulario de cada idioma\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF5VCFxK3oTH",
        "outputId": "1f4a4aca-7b8f-4478-9c7a-17719038fb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~¿\n",
            "!\"#$%&'()*+,-./:;<=>?@\\]^_`{|}~¿\n",
            "!\"#$%&'()*+,-./:;<=>?@\\^_`{|}~¿\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(train_spanish_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_DLNaOf35p6",
        "outputId": "f011d81c-6811-40bf-864b-ad86a816d70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] Obtuvo una Licenciatura en Sociología y Estudios Sociales (y un título profesional de trabajo social) en la Universidad de Keele, Reino Unido, en 1980, un Máster en ciencia en [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        # La oración de entrada en español \n",
        "        # no incluye el último token para \n",
        "        # mantener las entradas y los \n",
        "        # objetivos en la misma longitud.\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    # La frase objetivo en español está un \n",
        "    # paso por delante. Ambos siguen siendo \n",
        "    # de la misma longitud (20 palabras)\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    # Utilizamos el almacenamiento en caché en memoria \n",
        "    # para acelerar el preprocesamiento\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "Mef49Gdm4CLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unbb84zX4TfU",
        "outputId": "989fee45-8bff-46ff-ad0f-cf3ce1e37897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FUNCIONES SUELTAS**"
      ],
      "metadata": {
        "id": "u3HaLoIj5XBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "c276RwrM4ahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzIqOhq60H5m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Tamaño de los vectores de los tokens de entrada\n",
        "        self.embed_dim = embed_dim\n",
        "        # Tamaño de la capa densa interna\n",
        "        self.dense_dim = dense_dim\n",
        "        # Número de attention heads\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # El cálculo va en call()\n",
        "    def call(self, inputs, mask=None):\n",
        "        # La máscara que generará la capa Embedding \n",
        "        # será 2D, pero la capa de atención espera \n",
        "        # ser 3D o 4D, por lo que ampliamos su rango\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    # Implementamos la serialización para \n",
        "    # que podamos guardar el modelo\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de la entrada: (batch_size, sequence_length, embedding_dim)\n",
        "def layer_normalization(batch_of_sequences):\n",
        "    # Para calcular la media y la varianza, solo \n",
        "    # agrupamos datos sobre el último eje (eje -1)\n",
        "    mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    return (batch_of_sequences - mean) / variance"
      ],
      "metadata": {
        "id": "V8AfbC1CLw4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Y29YpdF15lj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de la entrada: (batch_size, height, width, channels)\n",
        "def batch_normalization(batch_of_images):\n",
        "    # Agrupa los datos sobre el eje del lote (eje 0), \n",
        "    # lo que crea interacciones entre las muestras en un lote.\n",
        "    mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "    variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "    return (batch_of_images - mean) / variance"
      ],
      "metadata": {
        "id": "zqCIRH0YMUW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AYrZ3220RW4"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        # Este atributo asegura que la capa propagará \n",
        "        # su máscara de entrada a sus salidas; el \n",
        "        # enmascaramiento en Keras es explícitamente \n",
        "        # opt-in. Si pasa una máscara a una capa que \n",
        "        # no implementa compute_mask() y que no expone \n",
        "        # este atributo support_masking, es un error.\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        # Genere una matriz de forma (sequence_length, sequence_length) \n",
        "        # con 1 en una mitad y 0 en la otra\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        # Lo replicamos a lo largo del eje del lote para obtener una matriz \n",
        "        # de forma  (batch_size, sequence_length, sequence_length)\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "bND78dATREAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call(self, inputs, encoder_outputs, mask=None):\n",
        "        # Recupera la máscara causal\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        # Prepara la máscara de entrada (que describe las \n",
        "        # ubicaciones de relleno en la secuencia de destino)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            # Fusiona las dos máscaras juntas\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            # Pasamos la máscara causal a la primera capa de atención, \n",
        "            # que realiza la self-attention sobre la secuencia de destino.\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            # Pasamos la máscara combinada a la segunda \n",
        "            # capa de atención, que relaciona la secuencia \n",
        "            # de origen con la secuencia de destino\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "jBxJmTwySHp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mFL075t37UuJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwdFFlly0RW4"
      },
      "source": [
        "**Transformer Extremo-a-Extremo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3EiJtWk0RW4"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "# Codificamos la oración fuente\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "# Codificamos la oración objetivo y la combinamos con la oración fuente codificada\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predecimos una palabra para cada posición de salida\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkMCBUmr0RW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c562ff-1381-4778-cb2b-6297256df735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "17/17 [==============================] - 62s 3s/step - loss: 7.9098 - accuracy: 0.0857 - val_loss: 6.7800 - val_accuracy: 0.1021\n",
            "Epoch 2/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 6.6307 - accuracy: 0.1264 - val_loss: 6.4377 - val_accuracy: 0.1612\n",
            "Epoch 3/30\n",
            "17/17 [==============================] - 49s 3s/step - loss: 6.2977 - accuracy: 0.1498 - val_loss: 6.2009 - val_accuracy: 0.1597\n",
            "Epoch 4/30\n",
            "17/17 [==============================] - 48s 3s/step - loss: 6.0015 - accuracy: 0.1671 - val_loss: 6.0327 - val_accuracy: 0.1768\n",
            "Epoch 5/30\n",
            "17/17 [==============================] - 49s 3s/step - loss: 5.7457 - accuracy: 0.1829 - val_loss: 5.9368 - val_accuracy: 0.1793\n",
            "Epoch 6/30\n",
            "17/17 [==============================] - 49s 3s/step - loss: 5.5007 - accuracy: 0.2066 - val_loss: 5.7626 - val_accuracy: 0.2029\n",
            "Epoch 7/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 5.3111 - accuracy: 0.2265 - val_loss: 5.9277 - val_accuracy: 0.1931\n",
            "Epoch 8/30\n",
            "17/17 [==============================] - 49s 3s/step - loss: 5.2263 - accuracy: 0.2252 - val_loss: 5.6638 - val_accuracy: 0.2187\n",
            "Epoch 9/30\n",
            "17/17 [==============================] - 49s 3s/step - loss: 4.7997 - accuracy: 0.2776 - val_loss: 5.5778 - val_accuracy: 0.2239\n",
            "Epoch 10/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 4.6302 - accuracy: 0.2867 - val_loss: 5.5766 - val_accuracy: 0.2174\n",
            "Epoch 11/30\n",
            "17/17 [==============================] - 48s 3s/step - loss: 4.5177 - accuracy: 0.2988 - val_loss: 5.7040 - val_accuracy: 0.2169\n",
            "Epoch 12/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 4.2371 - accuracy: 0.3215 - val_loss: 5.6554 - val_accuracy: 0.2184\n",
            "Epoch 13/30\n",
            "17/17 [==============================] - 46s 3s/step - loss: 4.0871 - accuracy: 0.3350 - val_loss: 5.7291 - val_accuracy: 0.2151\n",
            "Epoch 14/30\n",
            "17/17 [==============================] - 48s 3s/step - loss: 3.9992 - accuracy: 0.3416 - val_loss: 5.5792 - val_accuracy: 0.2242\n",
            "Epoch 15/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 3.7526 - accuracy: 0.3636 - val_loss: 5.5597 - val_accuracy: 0.2289\n",
            "Epoch 16/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 3.6349 - accuracy: 0.3728 - val_loss: 5.7349 - val_accuracy: 0.2114\n",
            "Epoch 17/30\n",
            "17/17 [==============================] - 49s 3s/step - loss: 3.4912 - accuracy: 0.3901 - val_loss: 5.6322 - val_accuracy: 0.2342\n",
            "Epoch 18/30\n",
            "17/17 [==============================] - 45s 3s/step - loss: 3.3127 - accuracy: 0.4151 - val_loss: 5.7007 - val_accuracy: 0.2294\n",
            "Epoch 19/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 3.1731 - accuracy: 0.4319 - val_loss: 5.5905 - val_accuracy: 0.2355\n",
            "Epoch 20/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 3.0558 - accuracy: 0.4498 - val_loss: 5.6783 - val_accuracy: 0.2267\n",
            "Epoch 21/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 2.8680 - accuracy: 0.4819 - val_loss: 5.5832 - val_accuracy: 0.2412\n",
            "Epoch 22/30\n",
            "17/17 [==============================] - 50s 3s/step - loss: 2.7773 - accuracy: 0.4896 - val_loss: 5.6153 - val_accuracy: 0.2477\n",
            "Epoch 23/30\n",
            "17/17 [==============================] - 48s 3s/step - loss: 2.6592 - accuracy: 0.5048 - val_loss: 5.6952 - val_accuracy: 0.2322\n",
            "Epoch 24/30\n",
            "17/17 [==============================] - 48s 3s/step - loss: 2.4863 - accuracy: 0.5337 - val_loss: 5.6762 - val_accuracy: 0.2430\n",
            "Epoch 25/30\n",
            "17/17 [==============================] - 48s 3s/step - loss: 2.3516 - accuracy: 0.5515 - val_loss: 5.7547 - val_accuracy: 0.2382\n",
            "Epoch 26/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 2.1954 - accuracy: 0.5743 - val_loss: 5.6803 - val_accuracy: 0.2480\n",
            "Epoch 27/30\n",
            "17/17 [==============================] - 47s 3s/step - loss: 2.0891 - accuracy: 0.5910 - val_loss: 5.7969 - val_accuracy: 0.2387\n",
            "Epoch 28/30\n",
            "17/17 [==============================] - 48s 3s/step - loss: 1.9983 - accuracy: 0.6044 - val_loss: 5.6912 - val_accuracy: 0.2558\n",
            "Epoch 29/30\n",
            "17/17 [==============================] - 46s 3s/step - loss: 1.8345 - accuracy: 0.6315 - val_loss: 5.8349 - val_accuracy: 0.2492\n",
            "Epoch 30/30\n",
            "17/17 [==============================] - 46s 3s/step - loss: 1.6885 - accuracy: 0.6598 - val_loss: 5.7429 - val_accuracy: 0.2578\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fddb8211520>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3H2505-0RW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b9cdd2-077b-4041-c0ec-7066021aa9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Elizaveta Petrovna Glinka (Russian: Елизаве́та Петро́вна Гли́нка, also known as Dr. Liza (Russian: До́ктор Ли́за); 20 February 1962 – 25 December 2016) was a Russian humanitarian worker and charity activist.\n",
            "[start] fue el primer ministro de la primera mujer de la banda de la salud mental de la banda de la\n",
            "-\n",
            "He currently sits on the board of Silkbank.\n",
            "[start] se graduó en la escuela secundaria de la región [end]\n",
            "-\n",
            "He became a naturalized citizen in 1934.\n",
            "[start] se convirtió en el primer ministro de la primera división [end]\n",
            "-\n",
            "Fukui was born in Osaka.\n",
            "[start] fue vicepresidente del boj entre 2002 [end]\n",
            "-\n",
            "His time of 47.93 seconds to win the 1990 European Athletics Championships was a British record, and he also won gold at the 1990 Commonwealth Games.\n",
            "[start] su madre era de la biblioteca de la biblioteca de la academia de la biblioteca de las políticas y la\n",
            "-\n",
            "He created history by holding three different designations in the same university as,Librarian of University of Ceylon (1971 to 1972)Librarian of Peradeniya Campus, University of Sri Lanka (1972 to 1978)Librarian University of Peradeniya through (1978 to 1979)\n",
            "[start] se formó en la banda de hans christian andersen para el distrito de la biblioteca de la vida en 1974\n",
            "-\n",
            "Éric Castonguay (born September 18, 1987) is a Canadian professional ice hockey player who is currently playing for HC Sierre-Anniviers in the Swiss League (SL), the second tier league of Switzerland.\n",
            "[start] se formó en el 15 de octubre de 2011 es una banda de la banda de 2018 renunció a la\n",
            "-\n",
            "In 2018, following her donation of $15 million, Concordia University renamed its faculty of engineering and computer science after her (the Gina Cody School of Engineering and Computer Science), making it the first university engineering faculty to be named after a woman in Canada, and internationally.\n",
            "[start] en 2018 después de la biblioteca de la región de la región de la biblioteca de 2018 [end]\n",
            "-\n",
            "She was one of three nurses from New Zealand in the Spanish Civil War, and married Willi Remmel a German anti-fascist volunteer in Spain in 1938.\n",
            "[start] se formó en el tres años de la asociación de la academia de la biblioteca de la compañía de la\n",
            "-\n",
            "When she was nine she danced at the Pantomime Theatre in the Tivoli Gardens.\n",
            "[start] cuando se formó en el premio de la región de la región de la biblioteca de la [end]\n",
            "-\n",
            "She later joined st. Mathias Kalemba senior secondary school for her A'level education and sat her Uganda Advanced Certificate of Education (UACE) in 1998.\n",
            "[start] se formó en la escuela secundaria de la escuela secundaria de biatlón de la biblioteca de la compañía de la\n",
            "-\n",
            "After graduating from university he worked as an economist in both the public and private sectors.\n",
            "[start] después de la universidad de nueva york en la región de la salud de salud pública en el [end]\n",
            "-\n",
            "The Queens line-up during this period usually comprised Hilda Tloubatla, Juliet Mazamisa, Ethel Mngomezulu, Nobesuthu Mbadu and Mildred Mangxola.\n",
            "[start] la biblioteca de las queens se vio inmersa en el presidente de la misma institución al finalizar el período de\n",
            "-\n",
            "After graduation from University of Düsseldorf, she worked as assistant physician, specialist and assistant medical director.\n",
            "[start] después de graduarse de la universidad de la institución de la región de la biblioteca de 2018 [end]\n",
            "-\n",
            "She has also won three gold medals as a member of the Dutch relay team at the European Championships.As of 2013, ter Mors has one ISU Short Track Speed Skating World Cup victory, coming as part of the Dutch relay team in 2012–13 at Dresden.\n",
            "[start] se ha casado 4 veces y en el primer ministro de la fundación en el campeonato mundial de la junta\n",
            "-\n",
            "He later also studied philosophy and medicine and graduated in medicine at about the age of 20.\n",
            "[start] se formó en la escuela secundaria en la escuela secundaria de la ciudad de la organización de la ciudad [end]\n",
            "-\n",
            "His passion was dancing, but he could not afford dance lessons.\n",
            "[start] su padre era de la salud pública fue la compañía de la salud [end]\n",
            "-\n",
            "He became a teacher of the national professional schools, and was appointed to a position in Armentières, Nord department.\n",
            "[start] se convirtió en la de la escuela de la biblioteca de la academia de la salud en el período de\n",
            "-\n",
            "He is now married to Eri Nakamura, a Batsheva dancer and costume designer with whom he has a daughter.\n",
            "[start] se casó con la escuela secundaria de la asociación de la compañía de la salud y en la copa mundial\n",
            "-\n",
            "Falcam served as the First Lady of the Federated States of Micronesia from 1999 to 2003 during the tenure of her husband, former President Leo Falcam.\n",
            "[start] desde marzo de la junta directiva de la asociación de la academia de la biblioteca de la compañía de la\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        # Muestra el siguiente token\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        # Convertimos la siguiente predicción del token en una \n",
        "        # cadena y la agregamos a la oración generada\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        # Condición de salida\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uRoF63xx5cxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}